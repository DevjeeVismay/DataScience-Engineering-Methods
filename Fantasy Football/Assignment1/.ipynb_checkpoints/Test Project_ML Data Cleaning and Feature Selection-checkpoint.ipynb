{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sci Eng Methods\n",
    "## Assignment 1 â€“ ML Data Cleaning and Feature Selection\n",
    "### ML Data Cleaning and Feature Selection\n",
    "\n",
    "\n",
    "In this assignment, you will use a dataset for predictive learning and check the quality of the data and determine which features are important.\n",
    "\n",
    "\n",
    "\n",
    "__Answer the following questions:__\n",
    "\n",
    "* What are the data types? (Only numeric and categorical)\n",
    "\n",
    "* Are there missing values?\n",
    "\n",
    "* What are the likely distributions of the numeric variables?\n",
    "\n",
    "* Which independent variables are useful to predict a target (dependent variable)? (Use at least three methods)\n",
    "\n",
    "* Which independent variables have missing data? How much? \n",
    "\n",
    "* Do the training and test sets have the same data?\n",
    "\n",
    "* In the predictor variables independent of all the other predictor variables?\n",
    "\n",
    "* Which predictor variables are the most important?\n",
    "\n",
    "* Do the ranges of the predictor variables make sense?\n",
    "\n",
    "* What are the distributions of the predictor variables?   \n",
    "\n",
    "* Remove outliers and keep outliers (does if have an effect of the final predictive model)?\n",
    "\n",
    "* Remove 1%, 5%, and 10% of your data randomly and impute the values back using at least 3 imputation methods. How well did the methods recover the missing values?  That is remove some data, check the % error on residuals for numeric data and check for bias and variance of the error.\n",
    "\n",
    "\n",
    "For categorical data, calculate the accuracy and a confusion matrix.\n",
    " \n",
    "\n",
    "Imputation Methods for Missing Data\n",
    "https://www.youtube.com/watch?v=fYhr8eF1uboLinks to an external site.\n",
    "\n",
    "\n",
    "Minimize Video\n",
    "Nice EDA notebook https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-pythonLinks to an external site.\n",
    "\n",
    "\n",
    "<img src=\"image.png\" alt=\"Local Image\" style=\"width:80%;\"/>\n",
    "\n",
    "\n",
    "__Scoring Rubric__\n",
    "\n",
    "1. Are my answers supported with data? (20 Points)\n",
    "\n",
    "  * Tables, graphs, and charts must support your evaluation/answers.\n",
    "\n",
    "  * It MUST run in Google Collab. You will also save the Google Collab notebook as a .ipynb notebook and upload that to Canvas . (5 Points)\n",
    "\n",
    " \n",
    "\n",
    "2. Public dataset (5 Points)\n",
    "\n",
    "  * Pick a public dataset that can be used for Regression or Classification. You MUST get approval for your dataset from the TAs.\n",
    "\n",
    " \n",
    "\n",
    "3. What code is yours and what have you adapted? (5 Points)\n",
    "\n",
    "  * You must explain what code you wrote and what you have done that is different. Failure to cite ANY code will result in a zero for this section.\n",
    "\n",
    " \n",
    "\n",
    "4. Did I explain my code clearly? (15 Points) Your code review score will be scaled to a range of 0 to 10 and be used for this score.\n",
    "\n",
    " \n",
    "\n",
    "5. Did I explain my licensing clearly? (5 Points) Failure to cite a clear license will result in a zero for this section.\n",
    "\n",
    " \n",
    "\n",
    "__Answers to listed questions (45 Points)__\n",
    "\n",
    " \n",
    "\n",
    "* Which independent variables are useful to predict a target (dependent variable)?\n",
    "\n",
    "\n",
    "* Which independent variable have missing data? How much? \n",
    "\n",
    "\n",
    "* Do the training and test sets have the same data?\n",
    "\n",
    "\n",
    "* In the predictor variables independent of all the other predictor variables?\n",
    "\n",
    "\n",
    "* Which predictor variables are the most important?\n",
    "\n",
    "\n",
    "* Do the ranges of the predictor variables make sense?\n",
    "\n",
    "\n",
    "* What are the distributions of the predictor variables?   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__Notes:__\n",
    "\n",
    "\n",
    "Normality - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n",
    "\n",
    "Homoscedasticity - I just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013)Links to an external site.. Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n",
    "\n",
    "Linearity- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n",
    "\n",
    "Absence of correlated errors - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Fantasy Football has evolved into a captivating realm within the sports community, enabling enthusiasts to craft their ideal teams and compete based on the actual performances of players in the English Premier League (EPL). This Jupyter Notebook project delves into the comprehensive Fantasy Premier League (FPL) dataset covering seasons 2016 to 2023, sourced from Fantasy-Premier-League.\n",
    "\n",
    "The dataset encompasses essential statistics for each player, including seasonal histories, positions, and team affiliations. Our primary objective is to conduct a thorough examination of the dataset, ensuring data quality, and pinpointing vital features crucial for refining Fantasy Football predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Overview\n",
    "\n",
    "The provided data set captures detailed information related to Fantasy Premier League (FPL) player statistics for seasons spanning 2016 to 2023. This dataset is sourced from Fantasy-Premier-League and encompasses diverse features that contribute to understanding player performance in the English Premier League.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
